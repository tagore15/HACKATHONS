{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DATA/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATORY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404290"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537932"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['qid1'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['qid1'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537933"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['qid2'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['qid2'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290654"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['qid1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_group = df.groupby(\"is_duplicate\")['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_duplicate\n",
       "0    255027\n",
       "1    149263\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2514d796d30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEHCAYAAABSjBpvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLss96obyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2514d7965c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_group.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS SOLUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4f102c615780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIN_FILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mis_dup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Atul\\Anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "IN_FILE = \"DATA/test.csv\"\n",
    "W_T = 3\n",
    "f = open('out.csv', 'w')\n",
    "f.write('test_id,is_duplicate\\n')\n",
    "\n",
    "with open(IN_FILE, \"r\", encoding='utf8') as test:\n",
    "    next(test)\n",
    "    for row in reader(test):\n",
    "        is_dup = 0\n",
    "        if (len(row) == 3):\n",
    "            if len(set(row[1].split()).intersection(set(row[2].split()))) > W_T:\n",
    "                is_dup = 1\n",
    "            f.write(\"%s,%d\\n\"%(row[0], is_dup))\n",
    "        else:\n",
    "            print(line)\n",
    "            print(row)\n",
    "            break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP WORDS FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "def filter_word(word_list):\n",
    "    return [word for word in word_list if word not in cachedStopWords]\n",
    "from csv import reader\n",
    "IN_FILE = \"DATA/test.csv\"\n",
    "W_T = 3\n",
    "f = open('out1.csv', 'w')\n",
    "f.write('test_id,is_duplicate\\n')\n",
    "\n",
    "with open(IN_FILE, \"r\", encoding='utf8') as test:\n",
    "    next(test)\n",
    "    for row in reader(test):\n",
    "        is_dup = 0\n",
    "        if (len(row) == 3):\n",
    "            if len(set(filter_word(row[1].split())).intersection(set(filter_word(row[2].split())))) > W_T:\n",
    "                is_dup = 1\n",
    "            f.write(\"%s,%d\\n\"%(row[0], is_dup))\n",
    "        else:\n",
    "            print(line)\n",
    "            print(row)\n",
    "            break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing on train set\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "def filter_word(word_list):\n",
    "    return [word for word in word_list if word not in cachedStopWords]\n",
    "from csv import reader\n",
    "IN_FILE = \"DATA/train.csv\"\n",
    "W_T = 3\n",
    "f = open('out_t.csv', 'w', encoding='utf8')\n",
    "f.write('test_id,is_duplicate\\n')\n",
    "\n",
    "with open(IN_FILE, \"r\", encoding='utf8') as test:\n",
    "    next(test)\n",
    "    for row in reader(test):\n",
    "        is_dup = 0\n",
    "        if (len(row) > 4):\n",
    "            #print(row)\n",
    "            if len(set(filter_word(row[3].split())).intersection(set(filter_word(row[4].split())))) > W_T:\n",
    "                is_dup = 1\n",
    "            f.write('\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%d\"\\n'%(row[0], row[1], row[2],row[3],row[4],is_dup))\n",
    "            #f.write(\"%s,%s,%s,%s,%s,%d\\n\"%(row[0], row[1], row[2],row[3].row[4],is_dup))\n",
    "        else:\n",
    "            #print(line)\n",
    "            print(row)\n",
    "            break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JACCARD SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")  + list(string.punctuation) + ['']\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "import re\n",
    "spw = ' |/|-|:|,|\"|\\'|\\(|\\)|\\.|&|#|\\?'\n",
    "\n",
    "def filter_word(word_list):\n",
    "    return [word for word in word_list if word not in cachedStopWords]\n",
    "from csv import reader\n",
    "IN_FILE = \"DATA/test.csv\"\n",
    "W_T = 3\n",
    "f = open('out6.csv', 'w', encoding='utf8')\n",
    "f.write('test_id,is_duplicate\\n')\n",
    "\n",
    "with open(IN_FILE, \"r\", encoding='utf8') as test:\n",
    "    next(test)\n",
    "    for row in reader(test):\n",
    "        is_dup = 0\n",
    "        if (len(row) == 3):\n",
    "            #q1 = filter_word(row[1].lower().translate(translator).split())\n",
    "            #q2 = filter_word(row[2].lower().translate(translator).split())\n",
    "            #q1 = filter_word(row[1].lower().split())\n",
    "            #q2 = filter_word(row[2].lower().split())\n",
    "            word1_list = filter_word(re.split(spw, row[1].lower()))\n",
    "            word2_list = filter_word(re.split(spw, row[2].lower()))\n",
    "            #print(word1_list)\n",
    "            q1 = filter_word([stemmer.stem(word) for word in word1_list])\n",
    "            q2 = filter_word([stemmer.stem(word) for word in word2_list])\n",
    "            \n",
    "            num_common_words = len(set(q1).intersection(set(q2)))\n",
    "            num_union_words = len(set(q1).union(set(q2)))\n",
    "            jac = (num_common_words*1.0)/(num_union_words+1)\n",
    "            if (jac > 0.4):\n",
    "                is_dup = 1\n",
    "            f.write(\"%s,%d\\n\"%(row[0], is_dup))\n",
    "            #f.write('\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%d\"\\n'%(row[0], row[1], row[2],row[3],row[4],is_dup))\n",
    "        else:\n",
    "            print(line)\n",
    "            print(row)\n",
    "            break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")  + list(punctuation)\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "def filter_word(word_list):\n",
    "    return [word for word in word_list if word not in cachedStopWords]\n",
    "from csv import reader\n",
    "IN_FILE = \"DATA/train.csv\"\n",
    "W_T = 3\n",
    "f = open('out_t2.csv', 'w', encoding='utf8')\n",
    "f.write('test_id,is_duplicate\\n')\n",
    "\n",
    "with open(IN_FILE, \"r\", encoding='utf8') as test:\n",
    "    next(test)\n",
    "    for row in reader(test):\n",
    "        is_dup = 0\n",
    "        if (len(row) > 4):\n",
    "            #num_common_words = len(set(filter_word(row[3].split())).intersection(set(filter_word(row[4].split()))))\n",
    "            #num_union_words = len(set(filter_word(row[3].split())).union(set(filter_word(row[4].split()))))\n",
    "            q1 = filter_word(stemmer.stem(row[3].lower().translate(translator).split()))\n",
    "            q2 = filter_word(stemmer.stem(row[4].lower().translate(translator).split())\n",
    "            num_common_words = len(set(q1).intersection(set(q2)))\n",
    "            num_union_words = len(set(q1).union(set(q2)))\n",
    "            jac = (num_common_words*1.0)/(num_union_words+1)\n",
    "            if (jac > 0.4):\n",
    "                is_dup = 1\n",
    "            #print(q1)\n",
    "            #f.write(\"%s,%d\\n\"%(row[0], is_dup))\n",
    "            f.write('\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%d\"\\n'%(row[0], row[1], row[2],row[3],row[4],is_dup))\n",
    "        else:\n",
    "            print(line)\n",
    "            print(row)\n",
    "            break\n",
    "        \n",
    "        #break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "cachedStopWords = stopwords.words(\"english\")  + list(punctuation)\n",
    "def filter_word(word_list):\n",
    "    return [word for word in word_list if word not in cachedStopWords]\n",
    "from csv import reader\n",
    "IN_FILE = \"DATA/test.csv\"\n",
    "\n",
    "from collections import defaultdict\n",
    "import math\n",
    "word_idf = defaultdict(lambda: 0)\n",
    "num_lines = 0\n",
    "with open(IN_FILE, \"r\", encoding='utf8') as test:\n",
    "    next(test)\n",
    "    for row in reader(test):\n",
    "        if (len(row) == 3):\n",
    "            q1 = filter_word(row[1].lower().translate(translator).split())\n",
    "            q2 = filter_word(row[2].lower().translate(translator).split())\n",
    "            \n",
    "            for q in q1:\n",
    "                word_idf[q] += 1\n",
    "            for q in q2:\n",
    "                word_idf[q] += 1\n",
    "            num_lines += 1\n",
    "        else:\n",
    "            print(line)\n",
    "            print(row)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118651"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5643"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idf['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1204"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idf['temple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7898"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idf['hyderabad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57689"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(([key for key in word_idf.keys() if word_idf[key] < 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "print(list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['axa',\n",
       " 'lnkd',\n",
       " 'iiftxatnmatsnap',\n",
       " 'cydonia',\n",
       " 'directlyindirectly',\n",
       " 'carrierdelivery',\n",
       " 'chopard',\n",
       " 'cartier',\n",
       " 'playstyle',\n",
       " 'fairimpartialblind',\n",
       " '➰',\n",
       " '180amper',\n",
       " 'chaged',\n",
       " 'alkhalid',\n",
       " 'indiaor',\n",
       " 'minored',\n",
       " 'boobsbreasts',\n",
       " 'gazillion',\n",
       " 'handsmale',\n",
       " 'makings',\n",
       " 'burara',\n",
       " 'crewrowing',\n",
       " 'dubra',\n",
       " 'chugged',\n",
       " 'withings',\n",
       " 'webfocus',\n",
       " 'byepass',\n",
       " 'stringbuffer',\n",
       " 'amaq',\n",
       " 'auditya',\n",
       " 'venkatesh',\n",
       " 'tnmc',\n",
       " 'classses',\n",
       " 'glomar',\n",
       " 'cmephod',\n",
       " 'taxability',\n",
       " 'kalindarobyn',\n",
       " '4monthold',\n",
       " 'vonrio’s',\n",
       " 'restraing',\n",
       " 'anythingwant',\n",
       " 'needlessly',\n",
       " 'technologyframework',\n",
       " 'bhorror',\n",
       " '1970present',\n",
       " '12809',\n",
       " 'studypoolcom',\n",
       " 'fatas',\n",
       " 'scarythrillerghost',\n",
       " 'storiesincidents',\n",
       " 'manudhane',\n",
       " 'ptedu',\n",
       " 'whiteamerican',\n",
       " 'hyperworks',\n",
       " 'winpcap',\n",
       " '413',\n",
       " 'naushad',\n",
       " 'aicet',\n",
       " 'aprovedcan',\n",
       " 'mtechcan',\n",
       " 'jobsexpecting',\n",
       " 'micrometer',\n",
       " 'brach',\n",
       " 'paramore',\n",
       " 'madre',\n",
       " 'smg900v',\n",
       " 'grasseating',\n",
       " 'corkscrew',\n",
       " 'caucasianmixed',\n",
       " '5875',\n",
       " 'tenens',\n",
       " 'paychecks',\n",
       " 'facist',\n",
       " 'shigatsu',\n",
       " 'uso',\n",
       " 'ndms',\n",
       " 'dingaling',\n",
       " '4point',\n",
       " 'bosphorus',\n",
       " 'nx7400',\n",
       " 'center’s',\n",
       " 'selangor',\n",
       " 'businessinvesting',\n",
       " 'tbt',\n",
       " 'overjoyed',\n",
       " 'sorghum',\n",
       " 'foodgrain',\n",
       " 'controle',\n",
       " 'gizmogazettenet',\n",
       " 'calabar',\n",
       " 'fangs',\n",
       " '3legged',\n",
       " 'phsically',\n",
       " 'fineyour',\n",
       " 'bmisused',\n",
       " 'coprincipal',\n",
       " 'copi',\n",
       " 'inclusions',\n",
       " 'chaebols',\n",
       " 'fullblooded',\n",
       " 'carryovers',\n",
       " 'narc',\n",
       " 'vetinari',\n",
       " 'civitas',\n",
       " '5536',\n",
       " 'seecs',\n",
       " 'ec21com',\n",
       " 'wiesner',\n",
       " '19851989',\n",
       " 'chipsets',\n",
       " 'fiestacc',\n",
       " 'costcopaymentprocessingcom',\n",
       " 'paymentscoopcom',\n",
       " 'pritish',\n",
       " '345tnt',\n",
       " 'ue4',\n",
       " 'clothesfood',\n",
       " 'processingmanufacture',\n",
       " 'consumptionweighted',\n",
       " 'meanread',\n",
       " '7885',\n",
       " '160km',\n",
       " '320km',\n",
       " 'palkhi',\n",
       " 'base2',\n",
       " 'base10',\n",
       " '4socket',\n",
       " 'x16',\n",
       " 'vindhyas',\n",
       " 'nonspherically',\n",
       " 'nuforce',\n",
       " '16000mah',\n",
       " 'moodswing',\n",
       " 'mathfracfracddx',\n",
       " 'mathfracddx',\n",
       " 'mathor',\n",
       " 'diplomacylike',\n",
       " 'olena',\n",
       " 'petrosyuk',\n",
       " 'portkey',\n",
       " 'sharetribe',\n",
       " 'bisexuallesbian',\n",
       " 'jalapeño',\n",
       " 'thyroiditis',\n",
       " 'hikingwalking',\n",
       " 'postnovember',\n",
       " 'epa',\n",
       " '06a',\n",
       " 'shiftinsert',\n",
       " 'gvim',\n",
       " 'minidrones',\n",
       " 'afbc',\n",
       " 'oompa',\n",
       " 'loompa',\n",
       " '110027',\n",
       " '110015',\n",
       " 'sumon',\n",
       " 'fimcity',\n",
       " 'learninggrowth',\n",
       " '5665658795',\n",
       " '5416707184828894751076611',\n",
       " 'vapouriser',\n",
       " 'mbapgdmfpm',\n",
       " 'sepcifically',\n",
       " 'cahnnel',\n",
       " 'sclorship',\n",
       " '3099',\n",
       " 'classb',\n",
       " 'indiapakisan',\n",
       " 'assignee',\n",
       " 'pmjjy',\n",
       " 'pmsby',\n",
       " 'sagitarrius',\n",
       " 'petco',\n",
       " 'selyse',\n",
       " 'chromiumcanary',\n",
       " '99pm',\n",
       " 'implanon',\n",
       " 'scarmarkscar',\n",
       " '40mg',\n",
       " 'fcn',\n",
       " 'musky',\n",
       " 'recive',\n",
       " 'visitiingvienna',\n",
       " 'foronedaytrip',\n",
       " 'reclamationisland',\n",
       " 'questioncentered',\n",
       " '“rogue',\n",
       " 'lopac',\n",
       " 'waurika',\n",
       " 'senatorial',\n",
       " '8word',\n",
       " '81000',\n",
       " 'californication',\n",
       " 'p80',\n",
       " 'gsk',\n",
       " 'zenify',\n",
       " 'canan',\n",
       " '96percent',\n",
       " 'i7–4720hq']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in word_idf.keys() if word_idf[key] < 10][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['max',\n",
       " 'min',\n",
       " 'none',\n",
       " 'car',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'she',\n",
       " 'he',\n",
       " 'h',\n",
       " 'ijk',\n",
       " '',\n",
       " 'k',\n",
       " 'lkdfhd',\n",
       " 'dfdsf',\n",
       " 'hello',\n",
       " 'yes',\n",
       " '']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"max/min none-car yes:no she,he,h(ijk)\\\"k'lkdfhd.dfdsf&hello#yes?\"\n",
    "#s.split(' ' + string.punctuation)\n",
    "\n",
    "import re\n",
    "spw = ' |/|-|:|,|\"|\\'|\\(|\\)|\\.|&|#|\\?'\n",
    "re.split(spw, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345796\n"
     ]
    }
   ],
   "source": [
    "print(num_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in word_idf:\n",
    "    word_idf[word] = np.log(num_lines/(word_idf[word]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hair', 'much', 'would', 'cost', 'best', 'way', 'money', 'start', 'two', 'one', 'black', 'books', 'time?', 'years', 'old', 'sex', 'year', 'app', 'social', 'made', 'people', '5', 'getting', '2', '3', 'learn', 'work?', 'get', 'buy', \"what's\", 'difference', 'could', 'still', 'like', 'trump', 'got', 'possible', 'person', 'want', 'eat', 'do?', 'win', 'why?', 'first', 'computer', 'language', 'programming', 'good', 'high', 'study', 'college', 'give', 'thing', 'you?', 'used', 'word', 'improve', 'many', 'us', '&', 'know', 'new', 'change', 'make', 'examples', 'question', \"i'm\", 'time', 'travel', 'job', 'web', 'ever', 'life?', 'write', 'business', 'ways', 'lose', 'weight?', 'movie', 'become', 'quora', 'quora?', 'better', 'watch', 'see', 'facebook', 'data', 'ask', 'find', 'questions', 'love', 'number', 'university', 'india?', 'feel', '?', 'without', 'life', 'say', \"don't\", 'mean?', 'really', 'use', 'going', 'bad', 'bank', 'back', 'happen', 'indian', 'top', 'mean', 'women', 'me?', 'student', 'go', 'earn', 'different', 'real', 'increase', 'android', 'anyone', 'it?', 'english', 'think', 'happens', 'war', 'india', 'need', 'software', 'things', 'world?', 'girl', 'weight', 'donald', 'phone', 'card', 'world', 'free', 'help', 'google', 'video', 'long', 'take', '1', 'mobile', 'prepare', 'tell', 'someone', \"doesn't\", 'game', 'using', 'stop', 'download', 'meaning', 'engineering', 'online', 'learning', 'rank', \"can't\", 'them?', 'name', 'day', 'iphone', 'book', 'movies', 'car', 'look', 'career', '2016', 'live', 'science', '10', 'right', 'online?', 'work', 'country', 'water', 'mechanical', '2016?', 'account', 'read', 'play', 'even', 'human', 'come', 'jee', 'english?', 'company', 'important', 'instagram', 'class', 'engineering?']\n"
     ]
    }
   ],
   "source": [
    "print(([key for key in word_idf.keys() if word_idf[key] < 5 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(([key for key in word_idf.keys() if word_idf[key] < 5 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6204090719964905"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idf['india']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9290032865682631"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idf['take']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-d2371402171c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_idf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'avg' is not defined"
     ]
    }
   ],
   "source": [
    "avg(word_idf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.791759469228055\n",
      "1.0986122886681098\n",
      "1.0986122886681098\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "from nltk.book import text1, text2, text3, text4, text5, text6\n",
    "\n",
    "mytexts = TextCollection([text1, text2, text3, text4, text5, text6])\n",
    "\n",
    "# Print the IDF of a word\n",
    "print(mytexts.idf(\"India\"))\n",
    "print(mytexts.idf(\"America\"))\n",
    "print(mytexts.idf(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    " \n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    " \n",
    "stop_words = stopwords.words('english') + list(punctuation)\n",
    " \n",
    "def tokenize(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.lower() for w in words]\n",
    "    return [w for w in words if w not in stop_words and not w.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51570 10788\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for file_id in reuters.fileids():\n",
    "    words = tokenize(reuters.raw(file_id))\n",
    "    vocabulary.update(words)\n",
    " \n",
    "vocabulary = list(vocabulary)\n",
    "word_index = {w: idx for idx, w in enumerate(vocabulary)}\n",
    " \n",
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "DOCUMENTS_COUNT = len(reuters.fileids())\n",
    " \n",
    "print(VOCABULARY_SIZE, DOCUMENTS_COUNT)      # 10788, 51581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.494430215031565\n",
      "3.612866417088128\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "word_idf = defaultdict(lambda: 0)\n",
    "for file_id in reuters.fileids():\n",
    "    words = set(tokenize(reuters.raw(file_id)))\n",
    "    for word in words:\n",
    "        word_idf[word] += 1\n",
    " \n",
    "for word in vocabulary:\n",
    "    word_idf[word] = math.log(DOCUMENTS_COUNT / float(1 + word_idf[word]))\n",
    " \n",
    "print(word_idf['deliberations'])     # 7.49443021503\n",
    "print(word_idf['committee'])     # 3.61286641709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.096534942233195\n",
      "4.076703531418199\n",
      "5.159055299214529\n",
      "6.647132354644362\n"
     ]
    }
   ],
   "source": [
    "print(word_idf['india'])\n",
    "print(word_idf['america'])\n",
    "print(word_idf['singapore'])\n",
    "print(word_idf['hospital'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.060443010546419\n",
      "8.18757739559151\n",
      "4.168195871842865\n",
      "0.4637201119906511\n",
      "3.6730615778715503\n",
      "7.67675177182552\n",
      "6.150695468330471\n",
      "['also', 'said', 'new', 'billion', 'last', 'dlrs', 'share', 'pct', '``', \"''\", 'mln', 'year', 'u.s.', 'lt', \"'s\", 'would', 'one', 'two', 'corp', 'net', 'company', 'vs', 'shr', 'note', 'inc', 'qtr', 'cts']\n"
     ]
    }
   ],
   "source": [
    "print(word_idf['tomorrow'])\n",
    "print(word_idf['bird'])\n",
    "print(word_idf['congress'])\n",
    "print(word_idf['said'])\n",
    "print(word_idf['received'])\n",
    "print(word_idf['listened'])\n",
    "print(word_idf['heard'])\n",
    "print(([key for key in word_idf.keys() if word_idf[key] < 2 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.096534942233195"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idf['india']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## COUNT VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "REGEX = re.compile(r\",\\s*\")\n",
    "def tokenize(text):\n",
    "    return [tok.strip().lower() for tok in REGEX.split(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(df['question1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['question1'].iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = vec.fit_transform(df['question1']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_freq = cv.fit_transform(df['question1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_freq[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_freq = tfidf.fit_transform(term_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_freq.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = (tfidf_freq.tocoo()).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_freq.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
